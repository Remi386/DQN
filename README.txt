    Суть процедуры Q-обучения заимствована из обучения с учителем.
Мы пытаемся аппроксимировать сложную нелинейную функцию Q(s,a) с помощью нейронной сети. 
Для этого мы должны вычислить цели для этой функции, используя уравнение Беллмана, 
а затем притвориться, что у нас есть проблема контролируемого обучения.  
Но одним из фундаментальных требований для оптимизации SGD является независимость 
обучающих данных и их одинаковое распределение, чему не удовлетворяют данные,
которые мы хотим использовать
    Чтобы справиться с этим, мы будем использовать большой буфер нашего 
прошлого опыта и выборки данных обучения вместо того, чтобы использовать наш последний опыт. 
    Шаги записываются в "эпизоды" - состояние, действие, награда, флаг конца эпизода, след. состояние
Эпизоды записываются в очередь определенной длины: старые записи выталкиваются новыми, более актуальными

Корреляция между шагами
    Уравнение Беллмана дает нам значение Q(s,a) через Q(s',a'). Однако между состояниями s и s' есть 
только один шаг, что делает их очень похожими, и нейронной сети очень сложно их различить. 
Когда мы выполняем обновление параметров нашей нейронной сети, чтобы приблизить Q (s,a) к 
желаемому результату, мы можем косвенно изменить значение, созданное для Q (s', a') и других 
состояний поблизости. Это может сделать наше обучение очень нестабильным. Чтобы сделать обучение 
более стабильным, существует трюк, называемый целевой сетью (tgt_net), с помощью которого мы сохраняем 
копию нашей сети и используем ее для значения Q (s', a') в уравнении Беллмана. Эта сеть синхронизируется 
с основной сетью только периодически, например, один раз за N шагов (где N обычно довольно большой 
гиперпараметр, такой как 1k или 10k итераций обучения).

Выбор действия зависит от номера текущего эпизода(e-greedy policy), чем больше номер, 
тем больше шанс выбрать действие осознанно. В самом начале все действия выбираются случайном образои.

Вычиление функции потерь: L = (Q(s, a) - r)^2 для конечных эпизодов, 
L = (Q(s, a) - (r + Gamma * max(Q(s', a')))^2 для остальных эпизодов, где Q(s, a) - 
q value для s состояния и a действия, s', a' - следующее состояние и действие соответсвенно.

Поведение самой среды остается идентичным с предыдущими заданиями.
В данной версии не реализовано ни случайное поведение кролика, ни поведение по определенному паттерну.

При разработке методов по улучшению обучения были придуманы три способа:
1) Записывать шаги не по одному, а все в рамках одного эпизода. Через среду передавать 
информацию о том, что кролик изменил свою позицию. Такие эпизоды записывать в 
отдельный буфер, который полностью используется в обучении.

2) Также записывать шаги в эпизоды, но также записывать и общую награду за эпизод.
Обучать нейросеть "лучшими" эпизодами по значению общей награды. При установке высокой
награды за поимку кролика, можно заменить первый метод.

3) Генерировать среду случайным образом. Обучать нейросеть на данной среде, пока не будет найден
лучший путь. Такой путь положить в отдельный буфер, который также будет участвовать в обучении.
Перегенерировать среду. Повторить. Навскидку самый ресурсоёмкий метод, но и самый гибкий.
(Проблема дупликатов? Сколько нужно итераций, чтобы покрыть все случаи расположения существ?
При изменении количества существ, такой метод также не будет работать)

Каждый из этих методов будет находиться в ветках metod_with_indication, sampling_metod, all_random
соответсвенно, до тех пор, пока не будет выбран лучший.
