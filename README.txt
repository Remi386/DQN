  Суть процедуры Q-обучения заимствована из обучения с учителем.
Мы пытаемся аппроксимировать сложную нелинейную функцию Q(s,a) с помощью нейронной сети. 
Для этого мы должны вычислить цели для этой функции, используя уравнение Беллмана, 
а затем притвориться, что у нас есть проблема контролируемого обучения.  
Но одним из фундаментальных требований для оптимизации SGD является независимость 
обучающих данных и их одинаковое распределение. В нашем случае 
данные, которые мы собираемся использовать для обновления SGD, не соответствуют следующим критериям: 
  1. Наши образцы не являются независимыми. Даже если мы накопим большой пакет выборок данных, 
все они будут очень близки друг к другу, поскольку принадлежат к одному эпизоду.
  2. Распределение наших обучающих данных не будет идентично выборкам, предоставленным оптимальной 
политикой, которую мы хотим изучить. Данные, которые у нас есть, будут результатом какой-то другой 
политики (нашей текущей политики, случайной или обеих в случае epsilon-greedy), но мы не хотим 
учиться играть в случайном порядке: мы хотим оптимальную политику с лучшей наградой.

  Чтобы справиться с этим, нам обычно нужно использовать большой буфер нашего 
прошлого опыта и выборки данных обучения вместо того, чтобы использовать наш последний опыт. 
Этот метод называется буфером воспроизведения. 
  Шаги записываются в "эпизоды" - состояние, действие, награда, флаг конца эпизода, след. состояние
Эпизоды записываются в очередь определенной длины: старые записи выталкиваются новыми, более актуальными
  Буфер воспроизведения позволяет нам тренироваться на более или менее независимых данных, но 
данные все еще будут достаточно свежими, чтобы тренироваться на выборках, сгенерированных нашей 
недавней политикой. В следующей главе мы проверим другой тип буфера воспроизведения: буфер 
с приоритетом, который обеспечивает более сложный подход к выборке.

Корреляция между шагами
 Уравнение Беллмана дает нам значение Q(s,a) через Q(s',a'). Однако между состояниями s и s' есть 
только один шаг, что делает их очень похожими, и нейронной сети очень сложно их различить. 
Когда мы выполняем обновление параметров нашей нейронной сети, чтобы приблизить Q (s,a) к 
желаемому результату, мы можем косвенно изменить значение, созданное для Q (s', a') и других 
состояний поблизости. Это может сделать наше обучение очень нестабильным. Чтобы сделать обучение 
более стабильным, существует трюк, называемый целевой сетью (tgt_net), с помощью которого мы сохраняем 
копию нашей сети и используем ее для значения Q (s', a') в уравнении Беллмана. Эта сеть синхронизируется 
с основной сетью только периодически, например, один раз за N шагов (где N обычно довольно большой 
гиперпараметр, такой как 1k или 10k итераций обучения).

Выбор действия зависит от номера текущего эпизода(e-greedy policy), чем больше номер, 
тем больше шанс выбрать действие осознанно. В самом начале все действия выбираются случайном образои.

Вычиление функции потерь: L = (Q(s, a) - r)^2 для конечных эпизодов, 
L = (Q(s, a) - (r + Gamma * max(Q(s', a')))^2 для остальных эпизодов, где Q(s, a) - 
q value для s состояния и a действия, s', a' - следующее состояние и действие соответсвенно.
