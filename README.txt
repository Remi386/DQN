    Суть процедуры Q-обучения заимствована из обучения с учителем.
Мы пытаемся аппроксимировать сложную нелинейную функцию Q(s,a) с помощью нейронной сети. 
Для этого мы должны вычислить цели для этой функции, используя уравнение Беллмана, 
а затем притвориться, что у нас есть проблема контролируемого обучения.  
Но одним из фундаментальных требований для оптимизации SGD является независимость 
обучающих данных и их одинаковое распределение, чему не удовлетворяют данные,
которые мы хотим использовать
    Чтобы справиться с этим, мы будем использовать большой буфер нашего 
прошлого опыта и выборки данных обучения вместо того, чтобы использовать наш последний опыт. 
    Шаги записываются в "эпизоды" - состояние, действие, награда, флаг конца эпизода, след. состояние
Эпизоды записываются в очередь определенной длины: старые записи выталкиваются новыми, более актуальными

Корреляция между шагами
    Уравнение Беллмана дает нам значение Q(s,a) через Q(s',a'). Однако между состояниями s и s' есть 
только один шаг, что делает их очень похожими, и нейронной сети очень сложно их различить. 
Когда мы выполняем обновление параметров нашей нейронной сети, чтобы приблизить Q (s,a) к 
желаемому результату, мы можем косвенно изменить значение, созданное для Q (s', a') и других 
состояний поблизости. Это может сделать наше обучение очень нестабильным. Чтобы сделать обучение 
более стабильным, существует трюк, называемый целевой сетью (tgt_net), с помощью которого мы сохраняем 
копию нашей сети и используем ее для значения Q (s', a') в уравнении Беллмана. Эта сеть синхронизируется 
с основной сетью только периодически, например, один раз за N шагов (где N обычно довольно большой 
гиперпараметр, такой как 1k или 10k итераций обучения).

Выбор действия зависит от номера текущего эпизода(e-greedy policy), чем больше номер, 
тем больше шанс выбрать действие осознанно. В самом начале все действия выбираются случайном образои.

Вычиление функции потерь: L = (Q(s, a) - r)^2 для конечных эпизодов, 
L = (Q(s, a) - (r + Gamma * max(Q(s', a')))^2 для остальных эпизодов, где Q(s, a) - 
q value для s состояния и a действия, s', a' - следующее состояние и действие соответсвенно.

Поведение самой среды остается идентичным с предыдущими заданиями.
В данной версии не реализовано ни случайное поведение кролика, ни поведение по определенному паттерну.
